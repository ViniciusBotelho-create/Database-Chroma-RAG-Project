{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca6ef3f-ec61-4c70-9b9b-017d32810685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import fitz\n",
    "import spacy\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import openai\n",
    "from pymongo import MongoClient\n",
    "from chromadb import Client as ChromaClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3246f5-e9b9-40c6-8f8e-728181b97b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o ChromaDB com persistência\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae66ff9c-b7e6-492f-9019-bb33625ccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexões com o mongodb\n",
    "mongo_client = MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"rag_db\"]\n",
    "mongo_chunks = mongo_db[\"chunks\"]\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"rag_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a119aff8-5a2b-437c-bccf-514b514c29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf...\n",
      "Processando: ./temp_pdfs/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf...\n",
      "Extração concluída para 1 documentos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "\n",
    "# Diretório temporário para armazenar PDFs baixados\n",
    "download_dir = './temp_pdfs/'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Carrega o modelo e aumenta o limite máximo de caracteres\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "nlp.max_length = 2_000_000\n",
    "\n",
    "# Palavras e frases a ignorar\n",
    "GENERIC_IGNORE_KEYWORDS = {\n",
    "    \"sumário\", \"índice\", \"resumo\", \"anexo\", \"figura\", \"tabela\", \"referência\", \"bibliografia\",\n",
    "    \"conteúdo\", \"protocolo\", \"gov\", \"secretaria\", \"assinatura\", \"documento assinado\",\n",
    "    \"orientações gerais\", \"preenchimento\", \"manual\", \"parecer técnico\"\n",
    "}\n",
    "\n",
    "PHRASE_BLACKLIST = [\n",
    "    r\"monitoramento de.*\\(e-cenários\\)\",\n",
    "    r\"^página \\d+.*\",\n",
    "    r\"^mapas\\s*-\\s*\",\n",
    "    r\"^inserção do estudo.*\",\n",
    "    r\"^um arquivo não substitui.*\",\n",
    "    r\"^o link para.*tipologia.*\",\n",
    "    r\"documentos, manifestações.*\",\n",
    "    r\"requerimento.*preenchido.*\",\n",
    "    r\"documento.*válido.*\"\n",
    "]\n",
    "\n",
    "KEY_VERBS = {\n",
    "    \"avaliar\", \"caracterizar\", \"delimitar\", \"analisar\", \"estimar\",\n",
    "    \"descrever\", \"propor\", \"identificar\", \"considerar\", \"indicar\",\n",
    "    \"demonstrar\", \"impactar\", \"recomendar\", \"quantificar\"\n",
    "}\n",
    "\n",
    "# Função para download de PDF\n",
    "def download_pdf(url, download_dir):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return filename\n",
    "    print(f\"Erro ao baixar {url}\")\n",
    "    return None\n",
    "\n",
    "# Função para verificar a presença de verbos-chave\n",
    "def contains_key_verb(sent):\n",
    "    return any(tok.lemma_ in KEY_VERBS for tok in nlp(sent))\n",
    "\n",
    "# Função para determinar a relevância da linha\n",
    "def is_line_irrelevant(line):\n",
    "    lower = line.lower()\n",
    "    if len(lower.strip()) < 15:\n",
    "        return True\n",
    "    if any(k in lower for k in GENERIC_IGNORE_KEYWORDS):\n",
    "        return True\n",
    "    if any(re.search(pattern, lower) for pattern in PHRASE_BLACKLIST):\n",
    "        return True\n",
    "    if re.search(r\"\\d{2}/\\d{2}/\\d{2,4}\", lower):\n",
    "        return True\n",
    "    if lower.endswith(\".pdf\"):\n",
    "        return True\n",
    "    if re.match(r\"^\\d+(\\.\\d+)+\\s+\", lower):\n",
    "        return True\n",
    "    if lower.isupper() and len(lower.split()) > 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Função para dividir texto em chunks\n",
    "def split_text(text, max_chars=500000):\n",
    "    chunks = []\n",
    "    while len(text) > max_chars:\n",
    "        split_point = text[:max_chars].rfind(\".\") + 1\n",
    "        if split_point < 50:\n",
    "            split_point = max_chars\n",
    "        chunks.append(text[:split_point].strip())\n",
    "        text = text[split_point:].strip()\n",
    "    if text:\n",
    "        chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "# Função para extrair texto relevante\n",
    "def extract_relevant_sentences(text, min_words=4):\n",
    "    relevant = []\n",
    "    chunks = split_text(text)\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        for sent in doc.sents:\n",
    "            s = sent.text.strip()\n",
    "            if len(s.split()) < min_words:\n",
    "                continue\n",
    "            if contains_key_verb(s):\n",
    "                relevant.append(s)\n",
    "            elif any(tok.pos_ == \"VERB\" for tok in sent):\n",
    "                relevant.append(s)\n",
    "    return relevant\n",
    "\n",
    "# Função para extrair texto limpo com spaCy\n",
    "def extract_clean_text_with_spacy(pdf_path, source_url):\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            raw_text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "        lines = raw_text.splitlines()\n",
    "        filtered = [line.strip() for line in lines if not is_line_irrelevant(line.strip())]\n",
    "        cleaned_text = \" \".join(filtered)\n",
    "        relevant_sentences = extract_relevant_sentences(cleaned_text)\n",
    "        return {\n",
    "            \"source_url\": source_url,\n",
    "            \"text\": \"\\n\".join(relevant_sentences) if relevant_sentences else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Lista de URLs\n",
    "document_urls = [\n",
    "    \"https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\"\n",
    "]\n",
    "\n",
    "# Processamento dos PDFs\n",
    "pdf_texts = {}\n",
    "\n",
    "for url in document_urls:\n",
    "    print(f\"Baixando: {url}...\")\n",
    "    pdf_path = download_pdf(url, download_dir)\n",
    "    if pdf_path:\n",
    "        print(f\"Processando: {pdf_path}...\")\n",
    "        pdf_data = extract_clean_text_with_spacy(pdf_path, url)\n",
    "        if pdf_data:\n",
    "            pdf_texts[os.path.basename(pdf_path)] = pdf_data\n",
    "\n",
    "print(f\"Extração concluída para {len(pdf_texts)} documentos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b24f620-ff48-43bb-914d-af724d217c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto do arquivo EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf dividido em 573 blocos e embeddings gerados.\n",
      "\n",
      "Os textos de 1 documentos foram particionados e processados em embeddings.\n",
      "Salvando documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf (573 chunks)\n",
      " -> Documento 'EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf' salvo com sucesso.\n",
      "\n",
      "\n",
      "573 chunks armazenados no MongoDB e no ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# Carregar a chave da API do ambiente\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Função para dividir os textos em chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Função para dividir os textos em chunks\n",
    "def split_text_into_chunks(text, chunk_size=512, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Função para gerar os embeddings de cada chunk de acordo com a formatação da OPENAI\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    return embeddings_model.embed_documents(chunks)\n",
    "\n",
    "# Gerando os chunks e embeddings para os textos extraídos dos documentos\n",
    "text_chunks = {}\n",
    "embeddings = {}\n",
    "\n",
    "for pdf_file, pdf_data in pdf_texts.items():\n",
    "    source_url = pdf_data.get(\"source_url\")\n",
    "    full_text = pdf_data.get(\"text\", \"\")\n",
    "\n",
    "    if not isinstance(full_text, str):\n",
    "        print(f\"Aviso: O texto extraído de {pdf_file} não é uma string. Convertendo para string vazia.\")\n",
    "        full_text = \"\"\n",
    "\n",
    "    text_chunks[pdf_file] = split_text_into_chunks(full_text)\n",
    "    embeddings[pdf_file] = generate_embeddings(text_chunks[pdf_file])\n",
    "\n",
    "    print(f\"Texto do arquivo {pdf_file} dividido em {len(text_chunks[pdf_file])} blocos e embeddings gerados.\")\n",
    "\n",
    "print(f\"\\nOs textos de {len(pdf_texts)} documentos foram particionados e processados em embeddings.\")\n",
    "\n",
    "# Configurando o ChromaDB com persistência\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Conexões com o MongoDB\n",
    "mongo_client = MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"rag_db\"]\n",
    "mongo_chunks = mongo_db[\"chunks\"]\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"rag_embeddings\")\n",
    "\n",
    "# Salvando dados processados no banco de dados\n",
    "total_chunks = 0\n",
    "\n",
    "for pdf_file in text_chunks:\n",
    "    source_url = pdf_texts[pdf_file].get(\"source_url\")\n",
    "    chunks = text_chunks[pdf_file]\n",
    "    vector_list = embeddings[pdf_file]\n",
    "\n",
    "    print(f\"Salvando documento: {pdf_file} ({len(chunks)} chunks)\")\n",
    "\n",
    "    for idx, (chunk_text, vector) in enumerate(zip(chunks, vector_list)):\n",
    "        chunk_id = f\"{pdf_file}_chunk_{idx}\"\n",
    "\n",
    "        # Verificar se o chunk já existe no MongoDB\n",
    "        if mongo_chunks.find_one({\"chunk_id\": chunk_id}):\n",
    "            print(f\" - Chunk '{chunk_id}' já existe no MongoDB. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Verificar se o ID já existe no ChromaDB\n",
    "        existing_ids = collection.get(ids=[chunk_id])\n",
    "        if existing_ids and existing_ids[\"ids\"]:\n",
    "            print(f\" - Chunk '{chunk_id}' já existe no ChromaDB. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        # Inserir no MongoDB\n",
    "        mongo_chunks.insert_one({\n",
    "            \"pdf_file\": pdf_file,\n",
    "            \"chunk_index\": idx,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"source_url\": source_url\n",
    "        })\n",
    "        print(f\"Embedding para {chunk_id}: {type(vector)} - {vector[:5]}\")  # Mostra os primeiros 5 elementos\n",
    "        # Inserir no ChromaDB\n",
    "        collection.add(\n",
    "            ids=[chunk_id],\n",
    "            embeddings=[vector],\n",
    "            metadatas=[{\n",
    "                \"chunk_index\": idx,\n",
    "                \"source_file\": pdf_file,\n",
    "                \"source_url\": source_url\n",
    "            }]\n",
    "        )\n",
    "\n",
    "        total_chunks += 1\n",
    "\n",
    "    print(f\" -> Documento '{pdf_file}' salvo com sucesso.\\n\")\n",
    "\n",
    "print(f\"\\n{total_chunks} chunks armazenados no MongoDB e no ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f973523-c9a8-4aa3-a382-362d79601523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de embeddings armazenados: 573\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = collection.get()\n",
    "print(f\"Total de embeddings armazenados: {len(all_embeddings['ids'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ebffd6-1913-4e49-9a74-4c3f921f31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado 1:\n",
      "Documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Chunk ID: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf_chunk_269\n",
      "Link: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Texto: Com uma área de 280.330 hectares, a APA inclui total ou parcialmente os municípios paulistas de Amparo, Bragança Paulista, Campinas, Holambra, Jaguariúna, Joanópolis, Mairiporã, Monte Alegre do Sul, Morungaba, Nazaré Paulista, Pedra Bela, Pedreira, Piracaia, Santo Antônio de Posse, Serra Negra, Soco...\n",
      "\n",
      "Resultado 2:\n",
      "Documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Chunk ID: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf_chunk_267\n",
      "Link: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Texto: De acordo com o Artigo 1º da Lei 7.438/91: Ficam declaradas Áreas de Proteção Ambiental - APA, regiões situadas em diversos municípios da bacia hidrográfica do rio Piracicaba e regiões da bacia do rio Juqueri- de constituir Zonas de Proteção aos Mananciais, respeitadas no que couber às respectivas l...\n",
      "\n",
      "Resultado 3:\n",
      "Documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Chunk ID: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf_chunk_263\n",
      "Link: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Texto: Dentre estas unidades de conservação, as Áreas de Proteção Ambiental – APAs destacam-se por serem também unidades de gestão integradas que buscam traduzir na prática o desafio do desenvolvimento sustentável, procurando harmonizar a conservação e a recuperação ambiental e as necessidades humanas.\n",
      "No ...\n",
      "\n",
      "Resultado 4:\n",
      "Documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Chunk ID: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf_chunk_270\n",
      "Link: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Texto: A APA foi instituída com a função de criar uma zona de proteção de mananciais nas áreas de drenagem referentes aos mananciais, cursos, reservatórios de água e demais recursos hídricos abrangidos pelas áreas especificadas no seu limite.\n",
      "A região envolve as sub-bacias dos rios Atibainha, Atibaia, Jagu...\n",
      "\n",
      "Resultado 5:\n",
      "Documento: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Chunk ID: EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf_chunk_273\n",
      "Link: https://cetesb.sp.gov.br/eiarima/eia/EIA-384-24-e-amb-90935-24-Unid-Proc-Oleos-Contam-PCBs-MG-Trafos-Pedreira.pdf\n",
      "Texto: A APA Estadual dos rios Piracicaba e Juqueri-Mirim, criada pelo Decreto Estadual no 26.882/87 e, posteriormente, regulamentada pela Lei Estadual no 7.438/91, envolve o extremo nordeste do Município de Campinas e tem por objetivo proteger áreas de cabeceiras e afluentes de alto curso da bacia hidrogr...\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "def test_query(query: str):\n",
    "    # Gerar o embedding da query\n",
    "    embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    # Executar a consulta no ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    # Verificar se houve retorno\n",
    "    if not results[\"metadatas\"]:\n",
    "        print(\"Nenhum resultado encontrado para a query.\")\n",
    "        return\n",
    "\n",
    "    # Exibir os resultados\n",
    "    for idx, metadata in enumerate(results[\"metadatas\"][0]):\n",
    "        chunk_id = metadata[\"source_file\"] + \"_chunk_\" + str(metadata[\"chunk_index\"])\n",
    "        print(f\"\\nResultado {idx + 1}:\")\n",
    "        print(f\"Documento: {metadata['source_file']}\")\n",
    "        print(f\"Chunk ID: {chunk_id}\")\n",
    "        print(f\"Link: {metadata['source_url']}\")\n",
    "\n",
    "        # Buscar o texto no MongoDB\n",
    "        chunk = mongo_chunks.find_one({\"chunk_id\": chunk_id})\n",
    "        if chunk:\n",
    "            print(f\"Texto: {chunk['chunk_text'][:300]}...\")  # Limitar a exibição a 300 caracteres\n",
    "\n",
    "# Exemplo de consulta\n",
    "test_query(\"Quais municipios estão incluidos no APA?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18a5aa0-e96f-4c12-9d4d-66551944f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'chunks' do MongoDB limpa com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# #limpar o banco de dadso caso necessário\n",
    "# # Conexão com o MongoDB\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# mongo_client = MongoClient(\"mongodb://localhost:27017\")\n",
    "# mongo_db = mongo_client[\"rag_db\"]\n",
    "# mongo_chunks = mongo_db[\"chunks\"]\n",
    "\n",
    "# # Remover todos os documentos da collection\n",
    "# mongo_chunks.delete_many({})\n",
    "# print(\"Coleção 'chunks' do MongoDB limpa com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f48988f-4beb-47c9-b911-755247b0eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removidos 573 documentos da coleção 'rag_embeddings'.\n"
     ]
    }
   ],
   "source": [
    "# #limpar o chromadb caso necessário\n",
    "# import chromadb\n",
    "\n",
    "# # Inicializar o cliente ChromaDB\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# collection = chroma_client.get_or_create_collection(name=\"rag_embeddings\")\n",
    "\n",
    "# # Buscar todos os IDs\n",
    "# all_ids = collection.get()[\"ids\"]\n",
    "\n",
    "# # Remover todos os documentos pelo ID\n",
    "# if all_ids:\n",
    "#     collection.delete(ids=all_ids)\n",
    "#     print(f\"Removidos {len(all_ids)} documentos da coleção 'rag_embeddings'.\")\n",
    "# else:\n",
    "#     print(\"Nenhum documento encontrado para remoção no ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3327ca90-72e1-48ba-b6b8-ca2676a7b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#estudar leitura de imagens (leitura de mapas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv311)",
   "language": "python",
   "name": "venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
