Para o desenvolvimento do projeto, foi utilizada a IDE Jupyter para a construção dos modelos de IA e dos devidos tratamentos necessários. Também foi utilizada a IDE Visual Studio Code para o desenvolvimento do front-end do projeto. Quanto às linguagens envolvidas, destacou-se o uso de React para a construção do front-end e de Python para o desenvolvimento da LLM, RAG, da aplicação dos multiagentes e para a extração de informações textuais e visuais presentes nas documentações analisadas.
O funcionamento da aplicação ocorreu em três distintos pontos de processamento. O primeiro foi o processamento de textos e documentos, cujo funcionamento deu-se por meio da biblioteca requests, com a qual uma lista de links de documentos selecionados foi verificada e baixada temporariamente. Utilizou-se as bibliotecas PyMuPDF, spaCy e funções básicas escritas em Python. Os textos desses documentos foram extraídos, filtrados e, com o uso da biblioteca LangChain, divididos em chunks, de forma já otimizada para posterior transformação em embeddings.
O modelo de transformação selecionado foi o text-embedding-ada-002, padrão da OpenAI, devido à LLM utilizada ser o modelo GPT-4 da mesma empresa. Ainda nesta etapa, foi realizada a conexão com dois bancos de dados: o MongoDB (via biblioteca pymongo) e o banco vetorial Chroma (via chromadb). No MongoDB, foram armazenados o nome do documento ao qual o chunk pertencia, o índice, o texto, o identificador e o link da fonte. No Chroma, armazenaram-se os embeddings, os metadados, os índices, o nome do documento de origem e o link da fonte.
O segundo módulo de desenvolvimento foi voltado para a análise de imagens. De modo semelhante ao anterior, uma lista de links com imagens selecionadas foi processada por meio das bibliotecas requests, PIL, io.BytesIO, transformers e PyTorch. As imagens foram classificadas como mapas, gráficos, diagramas, tabelas ou outros tipos. Com o uso adicional das bibliotecas numpy, cv2 e easyocr, extraiu-se a estrutura textual da imagem — como título, legenda e outros textos. Essas informações foram transformadas em embeddings (com o modelo all-MiniLM-L6-v2) e armazenadas no Chroma e no MongoDB, contendo o link da imagem, categoria, título, texto e embeddings.
O terceiro e último ponto de aplicação foi responsável pela organização dos multiagentes e pela interação com a LLM. Foi estabelecida uma conexão com os bancos de dados para recuperar as informações relevantes. Por meio da biblioteca LangGraph, os agentes foram organizados com diferentes funções. Inicialmente, definiu-se o estado do chat, com as variáveis: input, query, context, image_context, answer e validation, representando o estado de ação dos agentes.
O agente de busca textual foi responsável por transformar a entrada do usuário em embedding e realizar a busca por similaridade no banco Chroma. Os textos mais relevantes foram recuperados do MongoDB e adicionados à variável context, junto com o nome e o link dos documentos correspondentes. O agente de busca por imagens funcionou de forma análoga, adicionando conteúdo à variável image_context. Já o agente de geração de resposta comunicou-se com a API da OpenAI por meio de sua biblioteca, enviando a entrada do usuário ao modelo GPT-4, juntamente com o contexto textual, instruindo-o a agir como um assistente especialista em licenciamento ambiental, com o objetivo de auxiliar na elaboração de documentos EIA e RIMA.
Por fim, a resposta foi armazenada na variável answer, e o agente de validação a reenviou à API da OpenAI para análise e verificação da coerência da resposta com o contexto recuperado.
